# nlp-language-modeling

- Learn how to solve some common NLP problems with attention
  1. Tutorial on packing sequence in pytorch, BLEU score
  2. [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, ICLR 2015)] 
  3. [Attention is All you Need (Google Brain, NIPS 2017)]
  4. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)]
  5. [Korean BERT pre-trained cased (KoBERT)]
  
- Dependencies
  - Python 3.6+
  - PyTorch==1.3
  - Codes are heavily inspired by [https://bastings.github.io/], [https://github.com/scoutbee/], [https://nlp.seas.harvard.edu/], [https://mayhewsw.github.io/]

### Reference
1. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (Cho, 2014)]
2. [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, ICLR 2015)]
3. [Attention is All you Need (Google Brain, NIPS 2017)]
4. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)]

[https://bastings.github.io/]: https://bastings.github.io/annotated_encoder_decoder/
[Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, ICLR 2015)]: https://arxiv.org/abs/1409.0473
[https://github.com/scoutbee/]: https://github.com/scoutbee/pytorch-nlp-notebooks
[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (Cho, 2014)]: https://arxiv.org/pdf/1406.1078
[https://nlp.seas.harvard.edu/]: https://nlp.seas.harvard.edu/2018/04/03/attention.html
[Attention is All you Need (Google Brain, NIPS 2017)]: https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
[https://mayhewsw.github.io/]: https://mayhewsw.github.io/2019/01/16/can-bert-generate-text/
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)]: https://arxiv.org/abs/1810.04805
[Korean BERT pre-trained cased (KoBERT)]: https://github.com/SKTBrain/KoBERT
