{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Google-bert-multilingual pretrained-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "modelname = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained(modelname)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"안녕하세요. SK텔레콤에 가서 핸드폰을 개통할까요?\"\n",
    "target = '##통'\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = tokenized_text.index(target)\n",
    "tokenized_text[masked_index] = '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안', '##녕', '##하', '##세', '##요', '.', 'SK', '##텔', '##레', '##콤', '##에', '가', '##서', '핸', '##드', '##폰', '##을', '개', '[MASK]', '##할', '##까', '##요', '?']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9521, 118741, 35506, 24982, 48549, 119, 21275, 119354, 56645, 119309, 10530, 8843, 12424, 9962, 15001, 119401, 10622, 8857, 103, 14843, 118671, 48549, 136]\n"
     ]
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eos_index(tokenized_text):\n",
    "    end_marks = ['.', '?', '!']\n",
    "    idx_list=[]\n",
    "    for item in end_marks:        \n",
    "        try:\n",
    "            idx_list.append(tokenized_text.index(item))\n",
    "        except:\n",
    "            pass\n",
    "    return min(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_idx = eos_index(tokenized_text)\n",
    "\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "# this is for the first sentence. \n",
    "for i in range(eos_idx):\n",
    "    segments_ids[i] =0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ids = [1] * len(tokenized_text)\n",
    "mask_ids[masked_index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "masks_tensors = torch.tensor([mask_ids])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all tokens\n",
    "predictions = model(tokens_tensor, segments_tensors)\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item() # argmax로\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. SK텔레콤에 가서 핸드폰을 개통할까요?\n",
      "Masked: 안 ##녕 ##하 ##세 ##요 . SK ##텔 ##레 ##콤 ##에 가 ##서 핸 ##드 ##폰 ##을 개 [MASK] ##할 ##까 ##요 ?\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\", text)\n",
    "print(\"Masked:\", \" \".join(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: ['##량']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted token:\", predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 119547])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape # reconstruct 된 문장이므로 22개의 모든 token에 대한 probability를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([119547])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_masked_token = predictions[0, masked_index]\n",
    "prob_masked_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.6674, 15.1089, 14.6751, 14.3621, 13.9021, 13.8833, 13.7950, 13.7224,\n",
      "        13.4476, 13.3646], grad_fn=<TopkBackward>)\n",
      "tensor([ 44321,  12030,  20626,  61844,  58303, 119259,  18227,  42337, 118739,\n",
      "         21789])\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(prob_masked_token, 10)\n",
    "\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##량', '##인', '##조', '##별', '##입', '##척', '##화', '##방', '##념', '##개']\n"
     ]
    }
   ],
   "source": [
    "predicted_token = tokenizer.convert_ids_to_tokens(indices.numpy())\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_words(test_str):\n",
    "    text = test_str\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "    masked_index = tokenized_text.index('_')\n",
    "    tokenized_text[masked_index] = '[MASK]'\n",
    "    \n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences\n",
    "    eos_idx = eos_index(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    # this is for the first sentence 'hello'. \n",
    "    for i in range(eos_idx):\n",
    "        segments_ids[i] =0 \n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Predict all tokens\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    prob_masked_token = predictions[0, masked_index]\n",
    "\n",
    "    print(\"Original:\", text)\n",
    "    print(\"Masked:\", \" \".join(tokenized_text))\n",
    "    \n",
    "    values, indices = torch.topk(prob_masked_token, 10)\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(indices.numpy())\n",
    "    print('Predicted top-k tokens: ',  predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. 오늘 아침에 아주 맛있는 _을 먹었습니다.\n",
      "Masked: 안 ##녕 ##하 ##세 ##요 . 오 ##늘 아 ##침 ##에 아 ##주 맛 ##있는 [MASK] 을 먹 ##었 ##습 ##니다 .\n",
      "Predicted top-k tokens:  ['밥', '음', '[UNK]', '것', '맛', '점', '물', '겁', '음악', '빵']\n"
     ]
    }
   ],
   "source": [
    "generate_topk_words('안녕하세요. 오늘 아침에 아주 맛있는 _을 먹었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 만나서 반갑습니다. 오늘 아침에 재미있는 _을 읽었습니다.\n",
      "Masked: 만 ##나 ##서 반 ##갑 ##습 ##니다 . 오 ##늘 아 ##침 ##에 재 ##미 ##있는 [MASK] 을 읽 ##었 ##습 ##니다 .\n",
      "Predicted top-k tokens:  ['책', '글', '것', '[UNK]', '역', '점', '음악', '집', '말', '문']\n"
     ]
    }
   ],
   "source": [
    "generate_topk_words('만나서 반갑습니다. 오늘 아침에 재미있는 _을 읽었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 반가워요! 오늘 아침에 신나는 _를 들었습니다.\n",
      "Masked: 반 ##가 ##워 ##요 ! 오 ##늘 아 ##침 ##에 신 ##나는 [MASK] 를 들 ##었 ##습 ##니다 .\n",
      "Predicted top-k tokens:  ['노래', '\"', \"'\", '(', ')', '이야기', '!', '곡', '[UNK]', ',']\n"
     ]
    }
   ],
   "source": [
    "generate_topk_words('반가워요! 오늘 아침에 신나는 _를 들었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요? 저는 오늘 아_에 신나는 음악을 들었습니다.\n",
      "Masked: 안 ##녕 ##하 ##세 ##요 ? 저 ##는 오 ##늘 아 [MASK] 에 신 ##나는 음악 ##을 들 ##었 ##습 ##니다 .\n",
      "Predicted top-k tokens:  ['##프리카', '##침', '##시아', '.', '##이', '##역', '##래', '##리', '##날', '##늘']\n"
     ]
    }
   ],
   "source": [
    "generate_topk_words('안녕하세요? 저는 오늘 아_에 신나는 음악을 들었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 또 만났네요! 저는 _ 할 때 이 세상에서 제일 행복 합니다.\n",
      "Masked: 또 만 ##났 ##네 ##요 ! 저 ##는 [MASK] 할 때 이 세 ##상 ##에서 제 ##일 행 ##복 합 ##니다 .\n",
      "Predicted top-k tokens:  ['나', '[UNK]', '노래', '이', '행', '말', '복', '다', '모두', '함께']\n"
     ]
    }
   ],
   "source": [
    "generate_topk_words('또 만났네요! 저는 _ 할 때 이 세상에서 제일 행복 합니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Assign a score to a sentence\n",
    "- Evaluate whether a sentence is natural based on the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions=model(tensor_input)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(predictions.squeeze(),tensor_input.squeeze()).data \n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_perplexity(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions=model(tensor_input)\n",
    "    predictions = predictions.squeeze()\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    tensor_input = tensor_input.squeeze()\n",
    "    log_sum=0\n",
    "    for idx in range(len(tensor_input)):\n",
    "        log_sum += torch.log(predictions[idx, tensor_input[idx]])\n",
    "    \n",
    "    return math.exp(-log_sum/len(tensor_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The examples below are cherry-picked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.087662715284083"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('안녕하세요. 오늘 아침은 미세먼지가 많이 줄어 들어 맑은 하늘을 볼 수 있었습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKT-Brain KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "from kobert_transformers import get_kobert_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kobert_tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "kobert_lm_model = get_kobert_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안', '녕', '하세요', '.', '▁SK', '텔레콤', '에', '▁', '가', '서', '▁', '핸드', '폰', '을', '▁개통', '할', '까', '요', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"안녕하세요. SK텔레콤에 가서 핸드폰을 개통할까요?\"\n",
    "tokenized_text = kobert_tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "target = '▁개통'\n",
    "masked_index = tokenized_text.index(target)\n",
    "tokenized_text[masked_index] = '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. SK텔레콤에 가서 핸드폰을 개통할까요?\n",
      "Masked: ▁안 녕 하세요 . ▁SK 텔레콤 에 ▁ 가 서 ▁ 핸드 폰 을 [MASK] 할 까 요 ?\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\", text)\n",
    "print(\"Masked:\", \" \".join(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = kobert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences\n",
    "eos_idx = eos_index(tokenized_text)\n",
    "# this is for the first sentence. \n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "for i in range(eos_idx):\n",
    "    segments_ids[i] =0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentence A and B indices associated to 1st and 2nd sentences\n",
    "mask_ids = [1] * len(tokenized_text)\n",
    "# this is for the first sentence. \n",
    "mask_ids[masked_index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "masks_tensors = torch.tensor([mask_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kobert_lm_model(tokens_tensor, masks_tensors, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8002])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_masked_token = predictions[0][0][masked_index]\n",
    "prob_masked_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.1269,  8.3480,  7.9797,  7.6916,  7.6222,  6.8082,  6.5569,  6.5015,\n",
      "         6.4961,  6.3186], grad_fn=<TopkBackward>)\n",
      "tensor([2613, 1124, 1119,  856, 4758, 3334, 1111, 3219, 3726,  898])\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(prob_masked_token, 10)\n",
    "\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁사용', '▁구입', '▁구매', '▁개통', '▁통화', '▁연결', '▁교체', '▁얘기', '▁이용', '▁검색']\n"
     ]
    }
   ],
   "source": [
    "predicted_token = kobert_tokenizer.convert_ids_to_tokens(indices.numpy())\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kobert_generate_topk_words(test_str):\n",
    "    text = test_str\n",
    "    tokenized_text = kobert_tokenizer.tokenize(text)\n",
    "    \n",
    "    masked_index = tokenized_text.index('▁*')\n",
    "    tokenized_text[masked_index] = '[MASK]'\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = kobert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    eos_idx = eos_index(tokenized_text)\n",
    "\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    for i in range(eos_idx):\n",
    "        segments_ids[i] =0 \n",
    "        \n",
    "    mask_ids = [1] * len(tokenized_text)\n",
    "    mask_ids[masked_index] = 0\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    masks_tensors = torch.tensor([mask_ids])\n",
    "    \n",
    "    # Predict all tokens\n",
    "    predictions = kobert_lm_model(tokens_tensor, masks_tensors, segments_tensors)\n",
    "    prob_masked_token = predictions[0][0][masked_index]\n",
    "\n",
    "    print(\"Original:\", text)\n",
    "    print(\"Masked:\", \" \".join(tokenized_text))\n",
    "    \n",
    "    values, indices = torch.topk(prob_masked_token, 10)\n",
    "    predicted_token = kobert_tokenizer.convert_ids_to_tokens(indices.numpy())\n",
    "    print('Predicted top-k tokens: ',  predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. 오늘 아침에 아주 맛있는 *을 먹었습니다.\n",
      "Masked: ▁안 녕 하세요 . ▁오늘 ▁아침 에 ▁아주 ▁맛 있는 [MASK] 을 ▁먹 었습니다 .\n",
      "Predicted top-k tokens:  ['▁저녁', '▁밥', '▁음식', '▁아침', '▁죽', '것', '▁약', '▁술', '밥', '▁밤']\n"
     ]
    }
   ],
   "source": [
    "kobert_generate_topk_words('안녕하세요. 오늘 아침에 아주 맛있는 *을 먹었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. 오늘 *에 아주 맛있는 라면을 먹었습니다.\n",
      "Masked: ▁안 녕 하세요 . ▁오늘 [MASK] 에 ▁아주 ▁맛 있는 ▁ 라면 을 ▁먹 었습니다 .\n",
      "Predicted top-k tokens:  ['▁아침', '▁저녁', '밤', '▁낮', '▁밤', '만', '▁새벽', '▁오후', '▁오전', '▁처음']\n"
     ]
    }
   ],
   "source": [
    "kobert_generate_topk_words('안녕하세요. 오늘 *에 아주 맛있는 라면을 먹었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. 오늘 아침에 재미있는 *을 읽었습니다.\n",
      "Masked: ▁안 녕 하세요 . ▁오늘 ▁아침 에 ▁재미있 는 [MASK] 을 ▁읽 었습니다 .\n",
      "Predicted top-k tokens:  ['▁책', '▁소설', '▁신문', '글', '신문', '▁그림', '▁글', '▁시', '▁작품', '책']\n"
     ]
    }
   ],
   "source": [
    "kobert_generate_topk_words('안녕하세요. 오늘 아침에 재미있는 *을 읽었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 안녕하세요. 저는 *를 할 때 이 세상에서 제일 행복 합니다.\n",
      "Masked: ▁안 녕 하세요 . ▁저 는 [MASK] 를 ▁할 ▁때 ▁이 ▁세상 에서 ▁제 일 ▁행복 ▁합니다 .\n",
      "Predicted top-k tokens:  ['▁정치', '▁연애', '▁야구', '▁공부', '▁축구', '▁요리', '▁노래', '▁영화', '▁투표', '▁인터뷰']\n"
     ]
    }
   ],
   "source": [
    "kobert_generate_topk_words('안녕하세요. 저는 *를 할 때 이 세상에서 제일 행복 합니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kobert_get_score(sentence):\n",
    "    tokenize_input = kobert_tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([kobert_tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions=kobert_lm_model(tensor_input)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(predictions[0][0],tensor_input.squeeze()).data \n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.38208281770758"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('안녕하세요. 책상 위에 책이 있어서 읽어 보았습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3895272035424144"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('안녕하세요. 책상 위에 챙이 있어서 읽어 보았습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6909254883213238"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('안녕하세요. 아침으로 토스트를 먹었다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.093944511886682"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('안녕하세요. 아침으로 트스트를 먹었다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4528209396508616"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('안녕하세요. 동물원에서 코끼리를 봤습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.763551134568271"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('안녕하세요. 동물원에서 전시회를 봤습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.714837313945098"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('반가워. 아버지가 옷을 입으셨다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.007834004967372"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('반가워. 아버지가 옷를 입으셨다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.579315082849272"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_get_score('반가워. 아버지가 구두를 입으셨다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
